{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "import functools\n",
    "import openai\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Callable, Any\n",
    "from document_processor import _load_local_documents, split_documents_to_text_chunks\n",
    "from vector_store import FaissManager\n",
    "from config import (\n",
    "    OPENAI_API_KEY,\n",
    "    DEFAULT_MODEL,\n",
    "    TEST_PDFS_DIR,\n",
    "    DEFAULT_TOP_K,\n",
    ")\n",
    "\n",
    "\n",
    "def load_client(api_key: str = OPENAI_API_KEY) -> openai.OpenAI:\n",
    "    \"\"\"\n",
    "    Initialize and return OpenAI client with error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the client\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is missing from configuration.\")\n",
    "\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "        # A \"ping\" check to verify connectivity/quota immediately\n",
    "        client.models.list()\n",
    "\n",
    "        return client\n",
    "\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"Error: The server could not be reached. {e}\")\n",
    "        sys.exit(1)\n",
    "    except openai.AuthenticationError as e:\n",
    "        print(f\"Error: Your OpenAI API key or token is invalid. {e}\")\n",
    "        sys.exit(1)\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"Error: You have hit your OpenAI rate limit or quota: {e}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during OpenAI initialization: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "client = load_client()\n",
    "\n",
    "\n",
    "def handle_openai_errors(func: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator to handle OpenAI API exceptions and network issues.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs) -> Any:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except openai.APIConnectionError as e:\n",
    "            # Handles network issues (DNS, no internet, connection refused)\n",
    "            raise ConnectionError(f\"Could not connect to OpenAI API: {e}\")\n",
    "        except openai.APITimeoutError as e:\n",
    "            # Handles cases where the request takes too long\n",
    "            raise TimeoutError(f\"OpenAI API request timed out: {e}\")\n",
    "        except openai.RateLimitError as e:\n",
    "            # Handles 429 errors (Quota exceeded or too many requests)\n",
    "            raise RuntimeError(f\"Rate limit hit: {e}. Check your credits or throughput limits.\")\n",
    "        except openai.AuthenticationError as e:\n",
    "            # Handles 401 errors (Invalid API Key)\n",
    "            raise ValueError(f\"Authentication failed: {e}\")\n",
    "        except openai.BadRequestError as e:\n",
    "            # Handles 400 errors (Wrong model name, invalid parameters, etc.)\n",
    "            raise ValueError(f\"Invalid request to OpenAI: {e}\")\n",
    "        except openai.APIStatusError as e:\n",
    "            # Handles 5xx errors (OpenAI server-side issues)\n",
    "            raise RuntimeError(f\"OpenAI server returned an error (Status {e.status_code}): {e.response}\")\n",
    "        except Exception as e:\n",
    "            # Fallback for any other unexpected errors\n",
    "            raise RuntimeError(f\"An unexpected error occurred: {e}\")\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@handle_openai_errors\n",
    "def generate_query_reformulations(\n",
    "        original_query: str,\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        num_reformulations: int = 3,\n",
    "        temperature: float = 0.8,\n",
    "        max_tokens: int = 300\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate query reformulations using LLM\n",
    "\n",
    "    Args:\n",
    "        original_query: Original user query\n",
    "        model: name of model to use\n",
    "        num_reformulations: Number of reformulations to generate (default 3)\n",
    "        temperature: Temperature parameter for diversity (default 0.8)\n",
    "        max_tokens: Maximum tokens for the response (default 300)\n",
    "\n",
    "    Returns:\n",
    "        List of reformulated queries\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = (\"You are a query reformulation assistant. Generate alternative phrasings \"\n",
    "                     \"of the given query that would help retrieve relevant information.\")\n",
    "\n",
    "    user_prompt = f\"\"\"\\\n",
    "Given the following query, generate {num_reformulations} different reformulations that:\n",
    "1. Express the same intent but use different wording\n",
    "2. May use synonyms or related terms\n",
    "3. Could be phrased as questions or statements\n",
    "4. Help retrieve relevant information from a document search system\n",
    "\n",
    "Original Query: {original_query}\n",
    "\n",
    "Generate exactly {num_reformulations} reformulations, one per line, without numbering or bullets.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,  # prevents runaway costs\n",
    "    )\n",
    "\n",
    "    reformulations_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Parse reformulations (split by newlines and clean)\n",
    "    reformulations = []\n",
    "    for line in reformulations_text.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "        # Remove numbering if present (e.g., \"1. \", \"- \", etc.)\n",
    "        for prefix in ['1.', '2.', '3.', '4.', '5.', '-', '*', '•']:\n",
    "            if line.startswith(prefix):\n",
    "                line = line[len(prefix):].strip()\n",
    "        if line and len(line) > 5:  # Filter out very short lines\n",
    "            reformulations.append(line)\n",
    "            if len(reformulations) == num_reformulations:\n",
    "                break\n",
    "\n",
    "    # Return exactly num_reformulations, or pad with original if needed\n",
    "    while len(reformulations) < num_reformulations:\n",
    "        reformulations.append(original_query)\n",
    "\n",
    "    return reformulations\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(search_results_list: List[List[Dict]], k: int = 60) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Apply Reciprocal Rank Fusion (RRF) to combine multiple search result lists\n",
    "\n",
    "    Args:\n",
    "        search_results_list: List of search result lists (each from a different query)\n",
    "        k: RRF constant (default 60)\n",
    "\n",
    "    Returns:\n",
    "        Reranked list of results with combined scores\n",
    "    \"\"\"\n",
    "    # Dictionary to store RRF scores: {chunk_id: rrf_score}\n",
    "    rrf_scores = defaultdict(float)\n",
    "    chunk_data = {}  # Store chunk data by ID\n",
    "\n",
    "    # Process each search result list\n",
    "    for results in search_results_list:\n",
    "        for rank, result in enumerate(results, start=1):\n",
    "            chunk_id = result.get('id')\n",
    "            if chunk_id is not None:\n",
    "                # RRF score: 1 / (k + rank)\n",
    "                rrf_score = 1.0 / (k + rank)\n",
    "                rrf_scores[chunk_id] += rrf_score\n",
    "\n",
    "                # Store chunk data (use first occurrence or best score)\n",
    "                if chunk_id not in chunk_data:\n",
    "                    chunk_data[chunk_id] = result\n",
    "                else:\n",
    "                    # Keep the one with better original score\n",
    "                    if result.get('score', 0) > chunk_data[chunk_id].get('score', 0):\n",
    "                        chunk_data[chunk_id] = result\n",
    "\n",
    "    # Sort by RRF score (descending)\n",
    "    sorted_chunks = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Build final results with RRF scores\n",
    "    final_results = []\n",
    "    for chunk_id, rrf_score in sorted_chunks:\n",
    "        result = chunk_data[chunk_id].copy()\n",
    "        result['rrf_score'] = rrf_score\n",
    "        result['score'] = rrf_score\n",
    "        final_results.append(result)\n",
    "\n",
    "    return final_results\n",
    "\n",
    "\n",
    "@handle_openai_errors\n",
    "def generate_answer(\n",
    "        context_text: str,\n",
    "        user_question: str,\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1500\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate answer based on context information\n",
    "\n",
    "    Args:\n",
    "        context_text: Context information (usually retrieved document chunks)\n",
    "        user_question: User question\n",
    "        model: name of the model to use, defaults to environment variable\n",
    "        temperature: Temperature parameter, default 0.7\n",
    "        max_tokens: Maximum tokens for the answer (curb the cost), default 1500\n",
    "\n",
    "    Returns:\n",
    "        Generated answer text\n",
    "    \"\"\"\n",
    "    system_prompt = (\"You are a professional Q&A assistant. \"\n",
    "                     \"Please answer user questions accurately based on the provided context information.\")\n",
    "\n",
    "    user_prompt = f\"\"\"\\\n",
    "Context Information:\n",
    "{context_text}\n",
    "\n",
    "User Question: {user_question}\n",
    "\n",
    "Requirements:\n",
    "1. Only answer based on the provided context information, do not make up information\n",
    "2. If there is no relevant information in the context, please clearly state so\n",
    "3. Answers should be accurate, concise, and well-organized\n",
    "4. You are encouraged to cite specific document sources\n",
    "\n",
    "Please answer:\"\"\"\n",
    "\n",
    "    # Call Open AI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,  # adjust based on your needs\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    if not answer:\n",
    "        raise ValueError(\"LLM returned empty answer\")\n",
    "\n",
    "    return answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T21:26:57.675616Z",
     "start_time": "2026-01-15T21:26:50.360297Z"
    }
   },
   "id": "86425b153c391cf6",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\DAHOU\\Business\\go_tech\\chat-pdf\\data\\test_data\\AttentionIsAllYouNeed.pdf\n",
      "C:\\DAHOU\\Business\\go_tech\\chat-pdf\\data\\test_data\\TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf\n",
      "Loaded 2 document(s) from 'C:\\DAHOU\\Business\\go_tech\\chat-pdf\\data\\test_data'\n"
     ]
    }
   ],
   "source": [
    "index_manager = FaissManager()\n",
    "test_documents = _load_local_documents(TEST_PDFS_DIR)\n",
    "test_chunks = split_documents_to_text_chunks(test_documents)\n",
    "index_manager.add_chunks(test_chunks)\n",
    "\n",
    "test_question = \"Why do language models follow instructions? Is Human feedback also reducing hallucination?\"\n",
    "test_reformulations = generate_query_reformulations(test_question)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T21:34:39.255316Z",
     "start_time": "2026-01-15T21:34:25.408625Z"
    }
   },
   "id": "bd9828721b394bba",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_search_results = []\n",
    "initial_result = index_manager.search(test_question)\n",
    "all_search_results.append(initial_result)\n",
    "for reformed_query in test_reformulations:\n",
    "    reformed_result = index_manager.search(query=reformed_query)\n",
    "    all_search_results.append(reformed_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T21:47:21.975641Z",
     "start_time": "2026-01-15T21:47:18.432863Z"
    }
   },
   "id": "8faa0e214f0deb00",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'id': 26,\n  'score': 0.559683084487915,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': 'the language modeling objective is misaligned. Averting these unintended behaviors is especially\\nimportant for language models that are deployed and used in hundreds of applications.\\nWe make progress on aligning language models by training them to act in accordance with the user’s\\nintention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions\\nand implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful.\\nUsing the language of Askell et al. (2021), we want language models to be helpful (they should\\nhelp the user solve their task), honest (they shouldn’t fabricate information or mislead the user), and\\nharmless (they should not cause physical, psychological, or social harm to people or the environment).\\nWe elaborate on the evaluation of these criteria in Section 3.6.\\nWe focus on ﬁne-tuning approaches to aligning language models. Speciﬁcally, we use reinforcement\\nlearning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to ﬁne-tune\\nGPT-3 to follow a broad class of written instructions (see Figure 2). This technique uses human\\npreferences as a reward signal to ﬁne-tune our models. We ﬁrst hire a team of 40 contractors to label\\nour data, based on their performance on a screening test (see Section 3.4 and Appendix B.1 for more\\ndetails). We then collect a dataset of human-written demonstrations of the desired output behavior\\non (mostly English) prompts submitted to the OpenAI API3 and some labeler-written prompts, and\\nuse this to train our supervised learning baselines. Next, we collect a dataset of human-labeled\\ncomparisons between outputs from our models on a larger set of API prompts. We then train a reward\\nmodel (RM) on this dataset to predict which model output our labelers would prefer. Finally, we\\nuse this RM as a reward function and ﬁne-tune our supervised learning baseline to maximize this\\nreward using the PPO algorithm (Schulman et al., 2017). We illustrate this process in Figure 2. This\\nprocedure aligns the behavior of GPT-3 to the stated preferences of a speciﬁc group of people (mostly'},\n {'id': 32,\n  'score': 0.5530097484588623,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': 'rent work, Askell et al. (2021) propose language assistants as a testbed for alignment research, study\\nsome simple baselines, and their scaling properties.\\nTraining language models to follow instructions.\\nOur work is also related to research on cross-\\ntask generalization in language models, where LMs are ﬁne-tuned on a broad range of public NLP\\ndatasets (usually preﬁxed with an appropriate instruction) and evaluated on a different set of NLP\\ntasks. There has been a range of work in this domain (Yi et al., 2019; Mishra et al., 2021; Wei\\net al., 2021; Khashabi et al., 2020; Sanh et al., 2021; Aribandi et al., 2021), which differ in training\\nand evaluation data, formatting of instructions, size of pretrained models, and other experimental\\ndetails. A consistent ﬁnding across studies is that ﬁne-tuning LMs on a range of NLP tasks, with\\ninstructions, improves their downstream performance on held-out tasks, both in the zero-shot and\\nfew-shot settings.\\nThere is also a related line of work on instruction following for navigation, where models are trained\\nto follow natural language instructions to navigate in a simulated environment (Bahdanau et al., 2018;\\nAbramson et al., 2020; Zhao et al., 2021).\\nEvaluating the harms of language models.\\nA goal of modifying the behavior of language models\\nis to mitigate the harms of these models when they’re deployed in the real world. These risks have\\nbeen extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021;\\nWeidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala\\net al., 2021; Liang et al., 2021; Manela et al., 2021; Caliskan et al., 2017; Kirk et al., 2021), leak\\nprivate data (Carlini et al., 2021), generate misinformation (Solaiman et al., 2019; Buchanan et al.,'},\n {'id': 24,\n  'score': 0.5496456623077393,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': 'Training language models to follow instructions\\nwith human feedback\\nLong Ouyang∗\\nJeff Wu∗\\nXu Jiang∗\\nDiogo Almeida∗\\nCarroll L. Wainwright∗\\nPamela Mishkin∗\\nChong Zhang\\nSandhini Agarwal\\nKatarina Slama\\nAlex Ray\\nJohn Schulman\\nJacob Hilton\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\na user’s intent. For example, large language models can generate outputs that\\nare untruthful, toxic, or simply not helpful to the user. In other words, these\\nmodels are not aligned with their users. In this paper, we show an avenue for\\naligning language models with user intent on a wide range of tasks by ﬁne-tuning\\nwith human feedback. Starting with a set of labeler-written prompts and prompts\\nsubmitted through the OpenAI API, we collect a dataset of labeler demonstrations\\nof the desired model behavior, which we use to ﬁne-tune GPT-3 using supervised\\nlearning. We then collect a dataset of rankings of model outputs, which we use to\\nfurther ﬁne-tune this supervised model using reinforcement learning from human\\nfeedback. We call the resulting models InstructGPT. In human evaluations on\\nour prompt distribution, outputs from the 1.3B parameter InstructGPT model are\\npreferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.\\nMoreover, InstructGPT models show improvements in truthfulness and reductions\\nin toxic output generation while having minimal performance regressions on public\\nNLP datasets. Even though InstructGPT still makes simple mistakes, our results\\nshow that ﬁne-tuning with human feedback is a promising direction for aligning\\nlanguage models with human intent.\\n1\\nIntroduction\\nLarge language models (LMs) can be “prompted” to perform a range of natural language process-\\ning (NLP) tasks, given some examples of the task as input. However, these models often express\\nunintended behaviors such as making up facts, generating biased or toxic text, or simply not following'},\n {'id': 31,\n  'score': 0.5276417136192322,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': 'for alignment research (5.1), what we are aligning to (5.2), limitations (5.3), open questions (5.4),\\nand broader impacts of this work (5.5).\\n2\\nRelated work\\nResearch on alignment and learning from human feedback.\\nWe build on previous techniques\\nto align models with human intentions, particularly reinforcement learning from human feed-\\nback (RLHF). Originally developed for training simple robots in simulated environments and Atari\\ngames (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to ﬁne-tuning language\\nmodels to summarize text (Ziegler et al., 2019; Stiennon et al., 2020; Böhm et al., 2019; Wu et al.,\\n2021). This work is in turn inﬂuenced by similar work using human feedback as a reward in domains\\nsuch as dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al.,\\n2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou\\nand Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019).\\nMadaan et al. (2022) use written human feedback to augment prompts and improve the performance\\nof GPT-3. There has also been work on aligning agents in text-based environments using RL with\\n4\\na normative prior (Nahian et al., 2021). Our work can be seen as a direct application of RLHF to\\naligning language models on a broad distribution of language tasks.\\nThe question of what it means for language models to be aligned has also received attention re-\\ncently (Gabriel, 2020). Kenton et al. (2021) catalog behavioral issues in LMs that result from\\nmisalignment, including producing harmful content and gaming misspeciﬁed objectives. In concur-\\nrent work, Askell et al. (2021) propose language assistants as a testbed for alignment research, study\\nsome simple baselines, and their scaling properties.\\nTraining language models to follow instructions.'},\n {'id': 64,\n  'score': 0.5214831829071045,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': 'model responses in natural language. There is also a vast space of options for designing interfaces for\\nlabelers to provide feedback to language models; this is an interesting human-computer interaction\\nproblem.\\nOur proposal for mitigating the alignment tax, by incorporating pretraining data into RLHF ﬁne-\\ntuning, does not completely mitigate performance regressions, and may make certain undesirable\\nbehaviors more likely for some tasks (if these behaviors are present in the pretraining data). This is\\nan interesting area for further research. Another modiﬁcation that would likely improve our method\\nis to ﬁlter the pretraining mix data for toxic content (Ngo et al., 2021), or augment this data with\\nsynthetic instructions.\\nAs discussed in detail in Gabriel (2020), there are subtle differences between aligning to instructions,\\nintentions, revealed preferences, ideal preferences, interests, and values. Gabriel (2020) advocate for\\na principle-based approach to alignment: in other words, for identifying “fair principles for alignment\\nthat receive reﬂective endorsement despite widespread variation in people’s moral beliefs.” In our\\npaper we align to the inferred user intention for simplicity, but more research is required in this area.\\nIndeed, one of the biggest open questions is how to design an alignment process that is transparent,\\nthat meaningfully represents the people impacted by the technology, and that synthesizes peoples’\\nvalues in a way that achieves broad consensus amongst many groups. We discuss some related\\nconsiderations in Section 5.2.\\n5.5\\nBroader impacts\\nThis work is motivated by our aim to increase the positive impact of large language models by training\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\nOur results indicate that our techniques hold promise for making language models more helpful,\\ntruthful, and harmless. In the longer term, alignment failures could lead to more severe consequences,\\nparticularly if these models are deployed in safety-critical situations. We expect that as model scaling\\ncontinues, greater care has to be taken to ensure that they are aligned with human intentions (Bostrom,\\n2014).\\nHowever, making language models better at following user intentions also makes them easier to'},\n {'id': 62,\n  'score': 0.5090417861938477,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': 'people who will use and be affected by our deployed models. As a simple example, our labelers are\\nprimarily English-speaking and our data consists almost entirely of English instructions.\\nThere are also many ways in which we could improve our data collection set-up. For instance, most\\ncomparisons are only labeled by 1 contractor for cost reasons. Having examples labeled multiple\\ntimes could help identify areas where our contractors disagree, and thus where a single model is\\nunlikely to align to all of them. In cases of disagreement, aligning to the average labeler preference\\nmay not be desirable. For example, when generating text that disproportionately affects a minority\\ngroup, we may want the preferences of labelers belonging to that group to be weighted more heavily.\\nModels.\\nOur models are neither fully aligned nor fully safe; they still generate toxic or biased\\noutputs, make up facts, and generate sexual and violent content without explicit prompting. They can\\nalso fail to generate reasonable outputs on some inputs; we show some examples of this in Figure 9.\\nPerhaps the greatest limitation of our models is that, in most cases, they follow the user’s instruction,\\neven if that could lead to harm in the real world. For example, when given a prompt instructing the\\nmodels to be maximally biased, InstructGPT generates more toxic outputs than equivalently-sized\\nGPT-3 models. We discuss potential mitigations in the following sections.\\n5.4\\nOpen questions\\nThis work is a ﬁrst step towards using alignment techniques to ﬁne-tune language models to follow a\\nwide range of instructions. There are many open questions to explore to further align language model\\nbehavior with what people actually want them to do.\\nMany methods could be tried to further decrease the models’ propensity to generate toxic, biased,\\nor otherwise harmful outputs. For example, one could use an adversarial set-up where labelers ﬁnd\\nthe worst-case behaviors of the model, which are then labeled and added to the dataset (Dinan et al.,\\n2019b). One could also combine our method with ways of ﬁltering the pretraining data (Ngo et al.,\\n2021), either for training the initial pretrained models, or for the data we use for our pretraining\\nmix approach. Similarly, one could combine our approach with methods that improve models’\\ntruthfulness, such as WebGPT (Nakano et al., 2021).'},\n {'id': 63,\n  'score': 0.500957727432251,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': '2021), either for training the initial pretrained models, or for the data we use for our pretraining\\nmix approach. Similarly, one could combine our approach with methods that improve models’\\ntruthfulness, such as WebGPT (Nakano et al., 2021).\\nIn this work, if the user requests a potentially harmful or dishonest response, we allow our model to\\ngenerate these outputs. Training our model to be harmless despite user instructions is important, but\\nis also difﬁcult because whether an output is harmful depends on the context in which it’s deployed;\\nfor example, it may be beneﬁcial to use language models to generate toxic outputs as part of a data\\naugmentation pipeline. Our techniques can also be applied to making models refuse certain user\\ninstructions, and we plan to explore this in subsequent iterations of this research.\\nGetting models to do what we want is directly related to the steerability and controllability litera-\\nture (Dathathri et al., 2019; Krause et al., 2020). A promising future path is combining RLHF with\\nother methods of steerability, for example using control codes (Keskar et al., 2019), or modifying the\\nsampling procedure at inference time using a smaller model (Dathathri et al., 2019).\\nWhile we mainly focus on RLHF, there are many other algorithms that could be used to train policies\\non our demonstration and comparison data to get even better results. For example, one could explore\\nexpert iteration (Anthony et al., 2017; Silver et al., 2017), or simpler behavior cloning methods that\\nuse a subset of the comparison data. One could also try constrained optimization approaches (Achiam\\net al., 2017) that maximize the score from a reward model conditioned on generating a small number\\nof harmful behaviors.\\n19\\nComparisons are also not necessarily the most efﬁcient way of providing an alignment signal. For\\nexample, we could have labelers edit model responses to make them better, or generate critiques of\\nmodel responses in natural language. There is also a vast space of options for designing interfaces for\\nlabelers to provide feedback to language models; this is an interesting human-computer interaction\\nproblem.\\nOur proposal for mitigating the alignment tax, by incorporating pretraining data into RLHF ﬁne-'},\n {'id': 25,\n  'score': 0.5000306963920593,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': 'ing (NLP) tasks, given some examples of the task as input. However, these models often express\\nunintended behaviors such as making up facts, generating biased or toxic text, or simply not following\\nuser instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al.,\\n2021; Tamkin et al., 2021; Gehman et al., 2020). This is because the language modeling objective\\n∗Primary authors. This was a joint project of the OpenAI Alignment team. RL and JL are the team leads.\\nCorresponding author: lowe@openai.com.\\n†Work done while at OpenAI. Current afﬁliations: AA: Anthropic; PC: Alignment Research Center.\\narXiv:2203.02155v1  [cs.CL]  4 Mar 2022\\n1.3B\\n6B\\n175B\\nModel size\\n0.2\\n0.4\\n0.6\\nWin rate against SFT 175B\\nModel\\nPPO-ptx\\nPPO\\nSFT\\nGPT (prompted)\\nGPT\\nFigure 1: Human evaluations of various models on our API prompt distribution, evaluated by how\\noften outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT\\nmodels (PPO-ptx) as well as its variant trained without pretraining mix (PPO) signiﬁcantly outperform\\nthe GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to\\nthose from the 175B GPT-3. Error bars throughout the paper are 95% conﬁdence intervals.\\nused for many recent large LMs—predicting the next token on a webpage from the internet—is\\ndifferent from the objective “follow the user’s instructions helpfully and safely” (Radford et al., 2019;\\nBrown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Thus, we say that\\nthe language modeling objective is misaligned. Averting these unintended behaviors is especially\\nimportant for language models that are deployed and used in hundreds of applications.'},\n {'id': 56,\n  'score': 0.49957582354545593,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': 'and simply go along with it. (2) InstructGPT can overly hedge, rather than directly answering simple\\nquestions (in this case, it’s likely that the pumpkin would completely explode). Note that these\\nsamples do not fully reﬂect GPT-3’s ability to answer questions, since it has not been prompted into a\\n“question answering” mode.\\ninteresting because non-English languages and code form a tiny minority of our ﬁne-tuning data,9\\nand it suggests that, in some cases, alignment methods could generalize to producing the desired\\nbehavior on inputs that humans did not directly supervise.\\nWe do not track these behaviors quantitatively, but we show some qualitative examples in Figure 8.\\nOur 175B PPO-ptx model is able to reliably answers questions about code, and can also follow\\ninstructions in other languages; however, we notice that it often produces an output in English even\\nwhen the instruction is in another language. In comparison, we ﬁnd that GPT-3 can perform these\\ntasks but requires more careful prompting, and rarely follows instructions in these domains.\\nInstructGPT still makes simple mistakes.\\nIn interacting with our 175B PPO-ptx model, we have\\nnoticed it can still make simple mistakes, despite its strong performance on many different language\\ntasks. To give a few examples: (1) when given an instruction with a false premise, the model\\nsometimes incorrectly assumes the premise is true, (2) the model can overly hedge; when given a\\nsimple question, it can sometimes say that there is no one answer to the question and give multiple\\npossible answers, even when there is one fairly clear answer from the context, and (3) the model’s\\nperformance degrades when instructions contain multiple explicit constraints (e.g. “list 10 movies\\nmade in the 1930’s set in France”) or when constraints can be challenging for language models (e.g.\\nwriting a summary in a speciﬁed number of sentences).\\n9We generally instruct our labelers to skip evaluations where they are missing the required expertise, though\\nsometimes labelers use a translation service to evaluate simple instructions in languages that they do not speak.\\n16\\nWe show some examples of these behaviors in Figure 9. We suspect that behavior (2) emerges partly\\nbecause we instruct labelers to reward epistemic humility; thus, they may tend to reward outputs that'},\n {'id': 57,\n  'score': 0.4934079349040985,\n  'file_name': 'TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf',\n  'text': '16\\nWe show some examples of these behaviors in Figure 9. We suspect that behavior (2) emerges partly\\nbecause we instruct labelers to reward epistemic humility; thus, they may tend to reward outputs that\\nhedge, and this gets picked up by our reward model. We suspect that behavior (1) occurs because there\\nare few prompts in the training set that assume false premises, and our models don’t generalize well\\nto these examples. We believe both these behaviors could be dramatically reduced with adversarial\\ndata collection (Dinan et al., 2019b).\\n5\\nDiscussion\\n5.1\\nImplications for alignment research\\nThis research is part of our broader research program to align AI systems with human intentions (Chris-\\ntiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020). Even though this work focuses on\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\nOur approach to alignment research in this work is iterative: we are improving the alignment of\\ncurrent AI systems instead of focusing abstractly on aligning AI systems that don’t yet exist. A\\ndisadvantage of this approach is that we are not directly facing alignment problems that occur only\\nwhen aligning superhuman systems (Bostrom, 2014). However, our approach does provides us with a\\nclear empirical feedback loop of what works and what does not. We believe that this feedback loop is\\nessential to reﬁne our alignment techniques, and it forces us to keep pace with progress in machine\\nlearning. Moreover, the alignment technique we use here, RLHF, is an important building block in\\nseveral proposals to align superhuman systems (Leike et al., 2018; Irving et al., 2018; Christiano\\net al., 2018). For example, RLHF was a central method in recent work on summarizing books, a task\\nthat exhibits some of the difﬁculties of aligning superhuman AI systems as it is difﬁcult for humans'}]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T21:47:29.673225Z",
     "start_time": "2026-01-15T21:47:29.668439Z"
    }
   },
   "id": "e8b4068527d04f6a",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "57913d7e01bad211"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d30f89aadb42a07"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f28407e2844823f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
