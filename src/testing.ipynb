{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "import functools\n",
    "import openai\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "from typing import List, Dict, Callable, Any\n",
    "from document_processor import _load_local_documents, split_documents_to_text_chunks\n",
    "from vector_store import FaissManager\n",
    "from config import (\n",
    "    OPENAI_API_KEY,\n",
    "    DEFAULT_MODEL,\n",
    "    TEST_PDFS_DIR,\n",
    "    DEFAULT_TOP_K,\n",
    ")\n",
    "\n",
    "\n",
    "def load_client(api_key: str = OPENAI_API_KEY) -> openai.OpenAI:\n",
    "    \"\"\"\n",
    "    Initialize and return OpenAI client with error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the client\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is missing from configuration.\")\n",
    "\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "        # A \"ping\" check to verify connectivity/quota immediately\n",
    "        client.models.list()\n",
    "\n",
    "        return client\n",
    "\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"Error: The server could not be reached. {e}\")\n",
    "        sys.exit(1)\n",
    "    except openai.AuthenticationError as e:\n",
    "        print(f\"Error: Your OpenAI API key or token is invalid. {e}\")\n",
    "        sys.exit(1)\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"Error: You have hit your OpenAI rate limit or quota: {e}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during OpenAI initialization: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "client = load_client()\n",
    "\n",
    "\n",
    "def handle_openai_errors(func: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator to handle OpenAI API exceptions and network issues.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs) -> Any:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except openai.APIConnectionError as e:\n",
    "            # Handles network issues (DNS, no internet, connection refused)\n",
    "            raise ConnectionError(f\"Could not connect to OpenAI API: {e}\")\n",
    "        except openai.APITimeoutError as e:\n",
    "            # Handles cases where the request takes too long\n",
    "            raise TimeoutError(f\"OpenAI API request timed out: {e}\")\n",
    "        except openai.RateLimitError as e:\n",
    "            # Handles 429 errors (Quota exceeded or too many requests)\n",
    "            raise RuntimeError(f\"Rate limit hit: {e}. Check your credits or throughput limits.\")\n",
    "        except openai.AuthenticationError as e:\n",
    "            # Handles 401 errors (Invalid API Key)\n",
    "            raise ValueError(f\"Authentication failed: {e}\")\n",
    "        except openai.BadRequestError as e:\n",
    "            # Handles 400 errors (Wrong model name, invalid parameters, etc.)\n",
    "            raise ValueError(f\"Invalid request to OpenAI: {e}\")\n",
    "        except openai.APIStatusError as e:\n",
    "            # Handles 5xx errors (OpenAI server-side issues)\n",
    "            raise RuntimeError(f\"OpenAI server returned an error (Status {e.status_code}): {e.response}\")\n",
    "        except Exception as e:\n",
    "            # Fallback for any other unexpected errors\n",
    "            raise RuntimeError(f\"An unexpected error occurred: {e}\")\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@handle_openai_errors\n",
    "def generate_query_reformulations(\n",
    "        original_query: str,\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        num_reformulations: int = 3,\n",
    "        temperature: float = 0.8,\n",
    "        max_tokens: int = 300\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate query reformulations using LLM\n",
    "\n",
    "    Args:\n",
    "        original_query: Original user query\n",
    "        model: name of model to use\n",
    "        num_reformulations: Number of reformulations to generate (default 3)\n",
    "        temperature: Temperature parameter for diversity (default 0.8)\n",
    "        max_tokens: Maximum tokens for the response (default 300)\n",
    "\n",
    "    Returns:\n",
    "        List of reformulated queries\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = (\"You are a query reformulation assistant. Generate alternative phrasings \"\n",
    "                     \"of the given query that would help retrieve relevant information.\")\n",
    "\n",
    "    user_prompt = f\"\"\"\\\n",
    "Given the following query, generate {num_reformulations} different reformulations that:\n",
    "1. Express the same intent but use different wording\n",
    "2. May use synonyms or related terms\n",
    "3. Could be phrased as questions or statements\n",
    "4. Help retrieve relevant information from a document search system\n",
    "\n",
    "Original Query: {original_query}\n",
    "\n",
    "Generate exactly {num_reformulations} reformulations, one per line, without numbering or bullets.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,  # prevents runaway costs\n",
    "    )\n",
    "\n",
    "    reformulations_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Parse reformulations (split by newlines and clean)\n",
    "    reformulations = []\n",
    "    for line in reformulations_text.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "        # Remove numbering if present (e.g., \"1. \", \"- \", etc.)\n",
    "        for prefix in ['1.', '2.', '3.', '4.', '5.', '-', '*', 'â€¢']:\n",
    "            if line.startswith(prefix):\n",
    "                line = line[len(prefix):].strip()\n",
    "        if line and len(line) > 5:  # Filter out very short lines\n",
    "            reformulations.append(line)\n",
    "            if len(reformulations) == num_reformulations:\n",
    "                break\n",
    "\n",
    "    # Return exactly num_reformulations, or pad with original if needed\n",
    "    while len(reformulations) < num_reformulations:\n",
    "        reformulations.append(original_query)\n",
    "\n",
    "    return reformulations\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "        search_results_list: List[List[Dict]],\n",
    "        k: int = 60,\n",
    "        top_k: int = DEFAULT_TOP_K\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Apply Reciprocal Rank Fusion (RRF) to combine multiple search result lists\n",
    "\n",
    "    Args:\n",
    "        search_results_list: List of search result lists (each from a different query)\n",
    "        k: RRF constant (default 60)\n",
    "        top_k: Number of top results to return after reranking (default DEFAULT_TOP_K)\n",
    "\n",
    "    Returns:\n",
    "        Reranked list of results with combined scores\n",
    "    \"\"\"\n",
    "    # Dictionary to store RRF scores: {chunk_id: rrf_score}\n",
    "    rrf_scores = defaultdict(float)\n",
    "    chunk_data = {}  # Store chunk data by ID\n",
    "\n",
    "    # Process each search result list\n",
    "    for results in search_results_list:\n",
    "        for rank, result in enumerate(results, start=1):\n",
    "            chunk_id = result.get('id', None)\n",
    "            if chunk_id:\n",
    "                # RRF score: 1 / (k + rank)\n",
    "                rrf_score = 1.0 / (k + rank)\n",
    "                rrf_scores[chunk_id] += rrf_score\n",
    "\n",
    "                # Store chunk data (use first occurrence or best score)\n",
    "                if chunk_id not in chunk_data:\n",
    "                    chunk_data[chunk_id] = result\n",
    "                else:\n",
    "                    # Keep the one with better original score\n",
    "                    if result.get('score', 0) > chunk_data[chunk_id].get('score', 0):\n",
    "                        chunk_data[chunk_id] = result\n",
    "\n",
    "    # get top k results by RRF score (descending)\n",
    "    top_chunks = nlargest(top_k, rrf_scores.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Build final results with RRF scores\n",
    "    final_results = []\n",
    "    for chunk_id, rrf_score in top_chunks:\n",
    "        result = chunk_data[chunk_id].copy()\n",
    "        result['rrf_score'] = rrf_score\n",
    "        result['score'] = rrf_score\n",
    "        final_results.append(result)\n",
    "\n",
    "    return final_results\n",
    "\n",
    "\n",
    "def format_context(results: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format retrieved results into context string\n",
    "    \"\"\"\n",
    "    context_parts = [f\"[Chunk {i}] {r['text']}\" for i, r in enumerate(results, 1)]\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    return context\n",
    "\n",
    "\n",
    "@handle_openai_errors\n",
    "def generate_answer(\n",
    "        context_text: str,\n",
    "        user_question: str,\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1500\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate answer based on context information\n",
    "\n",
    "    Args:\n",
    "        context_text: Context information (usually retrieved document chunks)\n",
    "        user_question: User question\n",
    "        model: name of the model to use, defaults to environment variable\n",
    "        temperature: Temperature parameter, default 0.7\n",
    "        max_tokens: Maximum tokens for the answer (curb the cost), default 1500\n",
    "\n",
    "    Returns:\n",
    "        Generated answer text\n",
    "    \"\"\"\n",
    "    system_prompt = (\"You are a professional Q&A assistant. \"\n",
    "                     \"Please answer user questions accurately based on the provided context information.\")\n",
    "\n",
    "    user_prompt = f\"\"\"\\\n",
    "Context Information:\n",
    "{context_text}\n",
    "\n",
    "User Question: {user_question}\n",
    "\n",
    "Requirements:\n",
    "1. Only answer based on the provided context information, do not make up information\n",
    "2. If there is no relevant information in the context, please clearly state so\n",
    "3. Answers should be accurate, concise, and well-organized\n",
    "4. You are encouraged to cite specific document sources\n",
    "\n",
    "Please answer:\"\"\"\n",
    "\n",
    "    # Call Open AI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,  # adjust based on your needs\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    if not answer:\n",
    "        raise ValueError(\"LLM returned empty answer\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "@handle_openai_errors\n",
    "def condense_multi_turn_query(\n",
    "        conversation_history: List[Dict[str, str]],\n",
    "        current_question: str,\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        temperature: float = 0.2,\n",
    "        max_tokens: int = 200\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Condense a multi-turn conversation into a single standalone retrieval query.\n",
    "\n",
    "    Args:\n",
    "        conversation_history: List of {\"role\": \"user\"/\"assistant\", \"content\": \"...\"}\n",
    "                              Excludes the current user question.\n",
    "        current_question: The latest user question.\n",
    "        model: LLM model name\n",
    "        temperature: Low temperature for determinism, default 0.2\n",
    "        max_tokens: Token limit for safety\n",
    "\n",
    "    Returns:\n",
    "        A single condensed standalone query string\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are a search query condensation assistant.\n",
    "Your task is to rewrite the user's latest question into a SINGLE, \\\n",
    "standalone, explicit query suitable for document retrieval.\n",
    "\n",
    "\n",
    "Rules:\n",
    "1. Resolve all references (it, they, that, this, etc.) using conversation context\n",
    "2. Preserve technical accuracy and intent\n",
    "3. Do NOT answer the question\n",
    "4. Do NOT add new facts not stated or implied\n",
    "5. Optimize for semantic search, not chat\n",
    "6. Output ONLY the rewritten query\"\"\"\n",
    "\n",
    "    history_text = []\n",
    "    for msg in conversation_history:\n",
    "        role = msg.get(\"role\", \"\").capitalize()\n",
    "        content = msg.get(\"content\", \"\").strip()\n",
    "        if content:\n",
    "            history_text.append(f\"{role}: {content}\")\n",
    "    history_text = \"\\n\".join(history_text)\n",
    "\n",
    "    user_prompt = f\"\"\"\\\n",
    "Conversation History:\n",
    "{history_text}\n",
    "\n",
    "Latest User Question:\n",
    "{current_question}\n",
    "\n",
    "Standalone Search Query:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    condensed_query = response.choices[0].message.content.strip()\n",
    "\n",
    "    if not condensed_query:\n",
    "        raise ValueError(\"Failed to generate condensed query\")\n",
    "\n",
    "    return condensed_query"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T22:34:55.428249Z",
     "start_time": "2026-01-15T22:34:54.502034Z"
    }
   },
   "id": "86425b153c391cf6",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\DAHOU\\Business\\go_tech\\chat-pdf\\data\\test_data\\AttentionIsAllYouNeed.pdf\n",
      "C:\\DAHOU\\Business\\go_tech\\chat-pdf\\data\\test_data\\TrainingLanguageModelsToFollowInstructionsWithHumanFeedback.pdf\n",
      "Loaded 2 document(s) from 'C:\\DAHOU\\Business\\go_tech\\chat-pdf\\data\\test_data'\n"
     ]
    }
   ],
   "source": [
    "index_manager = FaissManager()\n",
    "test_documents = _load_local_documents(TEST_PDFS_DIR)\n",
    "test_chunks = split_documents_to_text_chunks(test_documents)\n",
    "index_manager.add_chunks(test_chunks)\n",
    "\n",
    "test_question = \"Why do language models follow instructions? Is Human feedback also reducing hallucination?\"\n",
    "test_reformulations = generate_query_reformulations(test_question)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T22:35:07.224876Z",
     "start_time": "2026-01-15T22:34:58.623739Z"
    }
   },
   "id": "bd9828721b394bba",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_search_results = []\n",
    "initial_result = index_manager.search(test_question)\n",
    "all_search_results.append(initial_result)\n",
    "for reformed_query in test_reformulations:\n",
    "    reformed_result = index_manager.search(query=reformed_query)\n",
    "    all_search_results.append(reformed_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T22:35:09.890036Z",
     "start_time": "2026-01-15T22:35:07.226596Z"
    }
   },
   "id": "8faa0e214f0deb00",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# rerank results with RRF\n",
    "reranked_results = reciprocal_rank_fusion(all_search_results, top_k=5)\n",
    "\n",
    "# Merge search results into context\n",
    "context = format_context(reranked_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T22:35:58.859367Z",
     "start_time": "2026-01-15T22:35:58.856086Z"
    }
   },
   "id": "e8b4068527d04f6a",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_answer = generate_answer(context_text=context, user_question=test_question)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T22:38:35.404507Z",
     "start_time": "2026-01-15T22:38:31.206568Z"
    }
   },
   "id": "57913d7e01bad211",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language models follow instructions primarily through a process called reinforcement learning from human feedback (RLHF). This approach fine-tunes models like GPT-3 by using human preferences as a reward signal to align their behavior with user intentions. The training involves collecting datasets of human-written demonstrations and comparisons of model outputs, which are then used to improve the model's ability to generate responses that align with specific instructions (Chunk 1).\n",
      "\n",
      "Regarding hallucination, the context indicates that while human feedback does help improve the truthfulness of model outputs and reduce toxic content, the models are still not fully aligned or free from errors. InstructGPT, a model fine-tuned with human feedback, has shown improvements in truthfulness and reductions in toxic outputs, but it still makes simple mistakes and can generate biased or harmful content if prompted (Chunk 5). Therefore, while human feedback does contribute to reducing hallucination, it does not eliminate it entirely.\n"
     ]
    }
   ],
   "source": [
    "print(test_answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T22:38:56.689769Z",
     "start_time": "2026-01-15T22:38:56.686214Z"
    }
   },
   "id": "d30f89aadb42a07",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Follow-up Question: So, hallucination reduction isn't the main progress, then what is the major achievement of this trainning method?\n",
      "\n",
      "Condensed Follow-up Query: What is the major achievement of reinforcement learning from human feedback (RLHF) in training language models?\n"
     ]
    }
   ],
   "source": [
    "test_followup_question = (\"So, hallucination reduction isn't the main progress, \"\n",
    "                          \"then what is the major achievement of this training method?\")\n",
    "conversation_history = [{\"role\": \"user\", \"content\": test_question}, {\"role\": \"assistant\", \"content\": test_answer}]\n",
    "condensed_query = condense_multi_turn_query(conversation_history, test_followup_question)\n",
    "print(\"\\nOriginal Follow-up Question:\", test_followup_question)\n",
    "print(\"\\nCondensed Follow-up Query:\", condensed_query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T22:52:13.620791Z",
     "start_time": "2026-01-15T22:52:12.697193Z"
    }
   },
   "id": "f28407e2844823f4",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "595b804c5df4dbb5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
